{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-25T11:40:44.562779Z",
     "start_time": "2025-02-25T11:40:44.553481Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "from docx import Document\n",
    "\n",
    "docx_folder = \"data\"\n",
    "output_json_file = \"dataset.json\"\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    content = \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n",
    "\n",
    "    # Separating story and storyboard content (you can adjust this logic based on the document's structure)\n",
    "    story_start = content.find(\"ORİJİNAL FIKRA\")\n",
    "    story_end = content.find(\"Karakter Özellikleri\")\n",
    "    storyboard_start = content.find(\"Sahne\")\n",
    "\n",
    "    # Extracting the story and storyboard parts\n",
    "    story = content[story_start:story_end].strip()  # Original Fable\n",
    "    storyboard = content[storyboard_start:].strip()  # Storyboard Content\n",
    "\n",
    "    return story, storyboard\n",
    "\n",
    "def load_all_documents(folder_path):\n",
    "    dataset = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".docx\"):  # Process only .docx files\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            story, storyboard = extract_text_from_docx(file_path)\n",
    "\n",
    "            # Create a structured entry with both story and storyboard content\n",
    "            entry = {\n",
    "                \"id\": filename.replace(\".docx\", \"\"),  # Use filename as ID\n",
    "                \"conversations\": [\n",
    "                    {\"from\": \"user\", \"value\": story},\n",
    "                    {\"from\": \"assistant\", \"value\": storyboard}\n",
    "                ]\n",
    "            }\n",
    "            dataset.append(entry)\n",
    "\n",
    "    return dataset"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T11:40:47.221602Z",
     "start_time": "2025-02-25T11:40:47.148719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load all documents and convert to structured format\n",
    "dataset = load_all_documents(docx_folder)\n",
    "\n",
    "# Save dataset to JSON\n",
    "with open(output_json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Dataset saved to {output_json_file}, with {len(dataset)} documents.\")\n",
    "\n",
    "# Save dataset as JSONL (newline-delimited JSON)\n",
    "with open(\"dataset.jsonl\", \"w\") as f:\n",
    "    for entry in dataset:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(\"Dataset saved to dataset.jsonl\")"
   ],
   "id": "1f0d062723bd6017",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to dataset.json, with 3 documents.\n",
      "Dataset saved to dataset.jsonl\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T11:49:32.470164Z",
     "start_time": "2025-02-25T11:49:31.277885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.cuda.empty_cache()  # This frees up unused memory\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, local_files_only=True)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, local_files_only=True, torch_dtype=\"auto\")\n"
   ],
   "id": "ce61780c75538c71",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5480060be66945c6a67d812997c2e15c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T11:49:35.291788Z",
     "start_time": "2025-02-25T11:49:34.682097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"dataset.json\")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ],
   "id": "bfe309be3c7b6db6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0a7b5fe09964d8280dd6ffb1b5b5688"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T11:49:38.786591Z",
     "start_time": "2025-02-25T11:49:38.724690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen_storyboard_model\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps=500,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=8,\n",
    ")\n",
    "\n",
    "print(\"TrainingArguments successfully initialized!\")"
   ],
   "id": "77f5d2cd8613ece8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments successfully initialized!\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T11:49:56.480985Z",
     "start_time": "2025-02-25T11:49:44.343178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"dataset.json\")\n",
    "\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "\n",
    "print(dataset)  # Should now show both 'train' and 'test'\n",
    "from transformers import Trainer, AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "if len(dataset['train']) > 1:  # Ensure there's more than one sample to split\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        batch_conversations = [\n",
    "            \" \".join([msg[\"value\"] for msg in conv]) for conv in examples[\"conversations\"]\n",
    "        ]\n",
    "        return tokenizer(batch_conversations, padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "    dataset = dataset['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "    train_dataset = dataset['train'].map(tokenize_function, batched=True)\n",
    "    eval_dataset = dataset['test'].map(tokenize_function, batched=True)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"Not enough samples to train. Skipping the training process.\")\n"
   ],
   "id": "eee6befe10925fb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'conversations'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'conversations'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e02cc6cf1058470684cdabd5a6f80114"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "54fd993d1d0f4117b98f8a4026cb89f4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T11:53:14.960572Z",
     "start_time": "2025-02-25T11:51:02.955016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.save_pretrained(\"./qwen_storyboard_model\")\n",
    "tokenizer.save_pretrained(\"./qwen_storyboard_model\")\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"./qwen_storyboard_model\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)  # Remove local_files_only=True if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "id": "33365c4141ff1701",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e7f570e466f4888ac6d82df911f553a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-02-26T13:27:41.781399100Z",
     "start_time": "2025-02-25T11:57:06.840404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"Create an storyboard in Turkish with given text after this and do not use images, instead explain these images as much as you can, tell what every character does, do not change original story just explain things happening scene by scene.Raw Story:\\n Nasreddin Hoca bir gün yolda giderken bir adamla karşılaşmış. Adamla sohbet etmeye başlamışlar. Bir saat havadan sudan konuştuktan sonra Hoca:– Kusura bakma arkadaş. Ben seni tanıyamadım, adın neydi?, diye sormuş.Adamcağız çok şaşırmış:– Madem beni tanımadın, neden benimle bir saattir sohbet ediyorsun?, demiş.Nasreddin Hoca:– Kıyafetlerin benimkine çok benziyordu. Ben de seni ben sandım, demiş. StoryBoard:\\n\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=4096, do_sample=True)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ],
   "id": "f327c528b767d8cb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
